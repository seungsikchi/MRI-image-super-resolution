from __future__ import print_function, division

from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate
from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add
from keras.layers.advanced_activations import LeakyReLU, ELU
from keras.layers import UpSampling2D, Conv2D, Conv2DTranspose, MaxPooling2D, GlobalAveragePooling2D, Reshape, Dense, AveragePooling2D
from keras.layers.merge import add, multiply
from layers import ReflectionPadding2D
from keras.models import Sequential, Model
from keras.initializers import RandomNormal
from tensorflow.keras.optimizers import Adam
from keras import backend as K

from tensorflow.keras.utils import plot_model
from glob import glob

from PIL import Image
import datetime
import matplotlib.pyplot as plt
import voxel 
import tensorflow as tf

import numpy as np
import os
import pickle as pkl
import random

from collections import deque

DATA_NAME = 'Brain'
PARAMETER = '0001_mri_512'
RUN_FOLDER = f'run/'
RUN_FOLDER += '_'.join([DATA_NAME, PARAMETER])
RUN_FOLDER += '/'
print(RUN_FOLDER)
mode = None

if not os.path.exists(RUN_FOLDER):
    # mkdir 함수는 해당 경로가 존재하지 않는다면(중간 경로는 필수로 있어야함) FileNotFoundError 에러가 발생.
    os.makedirs(RUN_FOLDER) # makedirs 함수는 에러 발생하지 않고 경로를 새로 만든다.
    os.mkdir(os.path.join(RUN_FOLDER, 'images'))
    os.mkdir(os.path.join(RUN_FOLDER, 'raws'))
    os.mkdir(os.path.join(RUN_FOLDER, 'model'))
    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))

    mode = 'bulid'

class CycleGAN():
    def __init__(self
        , Path
        , RUN_FOLDER
        , Epochs
        , input_dim
        , learning_rate
        , lambda_validation
        , lambda_reconstr
        , lambda_id
        , gen_n_filters
        , disc_n_filters
        , buffer_max_length = 50
        ):
        
        self.RUN_FOLDER = RUN_FOLDER
        self.PATH = Path
        self.input_dim = input_dim
        self.learning_rate = learning_rate
        self.buffer_max_length = buffer_max_length
        self.lambda_validation = lambda_validation
        self.lambda_reconstr = lambda_reconstr
        self.lambda_id = lambda_id

        self.gen_n_filters = gen_n_filters
        self.disc_n_filters = disc_n_filters

        # Input shape
        self.img_rows = input_dim[0]
        self.img_cols = input_dim[1]
        self.channels = input_dim[2]
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        
        self.original_image = []
        self.d_losses = []
        self.g_losses = []
        self.raw_A = []
        self.temp_R = []
        self.epoch = Epochs

        self.buffer_A = deque(maxlen = self.buffer_max_length)
        self.buffer_B = deque(maxlen = self.buffer_max_length)
        
        # Calculate output shape of D (PatchGAN)
        patch = int(self.img_rows / 2**3)
        self.disc_patch = (patch, patch, 1)

        self.weight_init = RandomNormal(mean=0., stddev=0.02)
        self.vx = voxel.PyVoxel()

        self.compile_models()

    
    def read_raw(self):
        path_imgA = self.PATH + '/EPI'
        path_imgB = self.PATH + '/Routine'

        raw_path_imgA = [ y for x in os.walk(path_imgA) for y in glob(os.path.join(x[0], '*.raw'))]
        raw_path_imgB = [ y for x in os.walk(path_imgB) for y in glob(os.path.join(x[0], '*.raw'))]


        imgs_A = []
        imgs_B = []

        for i in range(len(raw_path_imgA)):
            self.vx.ReadFromRaw(raw_path_imgA[i])
            self.original_image.append(self.vx.m_Voxel)
            self.vx.NormalizeMM()
            imgs_A.append(self.vx.m_Voxel)
            
        
        for i in range(len(raw_path_imgB)):
            self.vx.ReadFromRaw(raw_path_imgB[i])
            self.original_image.append(self.vx.m_Voxel)
            self.vx.NormalizeMM()
            imgs_B.append(self.vx.m_Voxel)
    

        imgs_A = np.array(imgs_A)
        imgs_B = np.array(imgs_B)
        print('image_x :', imgs_A.shape)
        print('image_y :', imgs_B.shape)

        return imgs_A, imgs_B

    
    def read_raw_one(self, path):
        imgs = []

        self.vx.ReadFromRaw(path)
        self.original_image.append(self.vx.m_Voxel)
        self.vx.NormalizeMM()
        imgs.append(self.vx.m_Voxel)
        imgs = np.array(imgs)

        self.image_processing(imgs,256)

        imgs = imgs[:22]

        return imgs
        
    def compile_models(self):

        # Build and compile the discriminators
        self.d_A = self.build_discriminator()
        self.d_B = self.build_discriminator()
        
        self.d_A.compile(loss='mse',
            optimizer=Adam(self.learning_rate, 0.5),
            metrics=['accuracy'])
        self.d_B.compile(loss='mse',
            optimizer=Adam(self.learning_rate, 0.5),
            metrics=['accuracy'])


        # Build the generators
        self.g_AB = self.build_generator_resnet()
        self.g_BA = self.build_generator_resnet()

        if mode == 'bulid': # mode가 'bulid' 즉, 처음 만들어졌다면 현재 상태 저장
            print('Save initial weights...')
            self.g_AB.save_weights(os.path.join(RUN_FOLDER + 'weights/g_AB_weights.h5'))
            self.g_BA.save_weights(os.path.join(RUN_FOLDER + 'weights/g_BA_weights.h5'))
            self.d_A.save_weights(os.path.join(RUN_FOLDER + 'weights/d_A_weights.h5'))
            self.d_B.save_weights(os.path.join(RUN_FOLDER + 'weights/d_B_weights.h5'))
        else:
            print('read weights...')
            self.g_AB.load_weights(os.path.join(RUN_FOLDER + 'weights/g_AB_weights.h5'))
            self.g_BA.load_weights(os.path.join(RUN_FOLDER + 'weights/g_BA_weights.h5'))
            self.d_A.load_weights(os.path.join(RUN_FOLDER + 'weights/d_A_weights.h5'))
            self.d_B.load_weights(os.path.join(RUN_FOLDER + 'weights/d_B_weights.h5'))

        # self.g_AB.summary()
        # self.g_BA.summary()

        # For the combined model we will only train the generators
        self.d_A.trainable = False
        self.d_B.trainable = False

        # Input images from both domains
        img_A = Input(shape=self.img_shape)
        img_B = Input(shape=self.img_shape)

        # Translate images to the other domain
        fake_B = self.g_AB(img_A)
        fake_A = self.g_BA(img_B)

        # Translate images back to original domain
        reconstr_A = self.g_BA(fake_B)
        reconstr_B = self.g_AB(fake_A)
        # Identity mapping of images
        img_A_id = self.g_BA(img_A)
        img_B_id = self.g_AB(img_B)

        # Discriminators determines validity of translated images
        valid_A = self.d_A(fake_A)
        valid_B = self.d_B(fake_B)

        # Combined model trains generators to fool discriminators
        self.combined = Model(inputs=[img_A, img_B],
                              outputs=[ valid_A, valid_B,
                                        reconstr_A, reconstr_B,
                                        img_A_id, img_B_id ])
        self.combined.compile(loss=['mse', 'mse',
                                    'mae', 'mae',
                                    'mae', 'mae'],
                            loss_weights=[  self.lambda_validation,                       self.lambda_validation,
                                            self.lambda_reconstr, self.lambda_reconstr,
                                            self.lambda_id, self.lambda_id ],
                            optimizer=Adam(0.0002, 0.5))

        self.d_A.trainable = True
        self.d_B.trainable = True
    

    def build_generator_resnet(self):

        def channel_squeeze(layer_input):
            conv_1 = Conv2D(1, (1,1), activation='sigmoid', kernel_initializer='he_normal', use_bias=False, padding='SAME')(layer_input)
            return multiply([layer_input, conv_1])

        def feature_aggregation_module(layer_input, filters):
            identity = layer_input
            conv1 = conv_block(layer_input, filters)
            resd1 = add([conv1, layer_input])

            conv2 = conv_block(resd1, filters)
            resd2 = add([conv2, resd1])

            conv3 = conv_block(resd2, filters)
            resd3 = add([conv3, resd2])

            conv4 = conv_block(resd3, filters)

            aggregated = Concatenate()([conv1, conv2, conv3, conv4])
            final = Conv2D(filters, (1,1), kernel_initializer='he_normal', use_bias=False, padding='SAME')(aggregated)
            final = LeakyReLU(alpha=0.3)(final)

            return final + identity

        def conv_block(layer_input, filters):
            conv_1 = Conv2D(filters, (3,3), kernel_initializer='he_normal', use_bias=True, padding='same')(layer_input)
            ac_1 = LeakyReLU(alpha=0.3)(conv_1)
            conv_2 = Conv2D(filters, (3,3), kernel_initializer='he_normal', use_bias=True, padding='same')(ac_1)
            SS1 = squeeze_excite_block(conv_2, filters)
            CS1 = channel_squeeze(conv_2)
            return add([SS1, CS1])

        def squeeze_excite_block(layer_input, filters):
            init = layer_input
            m1 = GlobalAveragePooling2D()(layer_input)
            squeeze = Reshape((1,1, filters))(m1)
            squeeze = Dense(filters // 16, activation='relu', kernel_initializer = 'he_normal', use_bias=False)(squeeze)
            excite = Dense(filters, activation='sigmoid', kernel_initializer = 'he_normal', use_bias=False)(squeeze)
            return multiply([init, excite])

        # Image input
        img = Input(shape=self.img_shape)

        y = img

        with tf.name_scope("Generator") as scope:

            lr_img = AveragePooling2D((4,4), strides=(4,4), padding = "same")(y)
            
            layer_11 = Conv2D(self.gen_n_filters, (9,9), kernel_initializer=self.weight_init, padding='SAME')(lr_img)
            encoded = LeakyReLU(alpha=0.3)(layer_11)

            layer21 = feature_aggregation_module(encoded, self.gen_n_filters)
            layer31 = feature_aggregation_module(layer21, self.gen_n_filters)

            layer41 = layer31 + encoded
            layer42 = Conv2D(self.gen_n_filters * (4 ** 2), (3,3), kernel_initializer=self.weight_init, padding='SAME')(layer41)
            layer43 = tf.nn.depth_to_space(layer42, 2)
            layer43 = LeakyReLU(alpha=0.3)(layer43)

            layer51 = Conv2D(self.gen_n_filters*4, (3,3), kernel_initializer=self.weight_init, padding='SAME')(layer43)
            layer52 = tf.nn.depth_to_space(layer51, 2)
            layer53 = LeakyReLU(alpha=0.3)(layer52)

            output_node = Conv2D(1, (9,9), padding='SAME', activation = None)(layer53)

        return Model(y, output_node)


    def build_discriminator(self):
        def classifier(input_layer, filters, strides, norm=True):
            c = Conv2D(filters, kernel_size=3, kernel_initializer='he_normal', strides=strides, padding='same')(input_layer)

            if norm:
                c = BatchNormalization()(c)
            
            c = LeakyReLU(0.2)(c)

            return c
        input = Input(shape=(256, 256,1))

        y = classifier(input, self.disc_n_filters, strides=2, norm=False) # 256 -> 128
        y = classifier(y, self.disc_n_filters*2, strides=2) # 128 -> 64
        y = classifier(y, self.disc_n_filters*4, strides=2) # 64 -> 32
        y = classifier(y, self.disc_n_filters*8, strides=2) # 32 -> 16

        output = Conv2D(1, kernel_size=3, strides=1, padding='same', activation='sigmoid')(y) # 16

        return Model(input, output)

    def train_discriminators(self, imgs_A, imgs_B, valid, fake):

        # Translate images to opposite domain
        fake_B = self.g_AB.predict(imgs_A)
        fake_A = self.g_BA.predict(imgs_B)

        self.buffer_B.append(fake_B)
        self.buffer_A.append(fake_A)

        fake_A_rnd = random.sample(self.buffer_A, min(len(self.buffer_A), len(imgs_A)))
        fake_B_rnd = random.sample(self.buffer_B, min(len(self.buffer_B), len(imgs_B)))

        # Train the discriminators (original images = real / translated = Fake)
        dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)
        dA_loss_fake = self.d_A.train_on_batch(fake_A_rnd, fake)
        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)

        dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)
        dB_loss_fake = self.d_B.train_on_batch(fake_B_rnd, fake)
        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)

        # Total disciminator loss
        d_loss_total = 0.5 * np.add(dA_loss, dB_loss)

        self.d_A.trainable = False
        self.d_B.trainable = False

        return (
            d_loss_total[0]
            , dA_loss[0], dA_loss_real[0], dA_loss_fake[0]
            , dB_loss[0], dB_loss_real[0], dB_loss_fake[0]
            , d_loss_total[1]
            , dA_loss[1], dA_loss_real[1], dA_loss_fake[1]
            , dB_loss[1], dB_loss_real[1], dB_loss_fake[1]
        )

    def train_generators(self, imgs_A, imgs_B, valid):

        # Train the generators
        return self.combined.train_on_batch([imgs_A, imgs_B],
                                                [valid, valid,
                                                imgs_A, imgs_B,
                                                imgs_A, imgs_B])


    def train(self, batch_size=1, sample_interval=1):
        

        start_time = datetime.datetime.now()

        batch_size = 1 #일반적으로 CycleGAN의 batch 사이즈는 1임
        patch = int(self.img_rows/2**4)
        self.disc_patch = (patch, patch, 1)

        valid = np.ones((batch_size,) + self.disc_patch)  #진짜 이미지들은 타깃이 1
        fake = np.zeros((batch_size,) + self.disc_patch)  #가짜 이미지들은 타깃이 0
        
        for epoch in range(self.epoch):
            for batch_i, routine_imgs in enumerate(self.train_y):
                count = 0
                channel = 20
                for imgs_channel, imgs_B in enumerate(routine_imgs):
                    
                    imgs_A = self.image_processing(self.train_x[batch_i][imgs_channel+4], 256)
                    imgs_B = self.image_processing(imgs_B, n_dim = 256)


                    #생성자를 사용하여 가짜 이미지 배치 생성
                    fake_B = self.g_AB.predict(imgs_A)
                    fake_A = self.g_BA.predict(imgs_B)

                    self.d_A.trainable = True
                    self.d_B.trainable = True
                    
                    #가짜 이미지와 진짜 이미지 배치로 각 판별자를 훈련
                    dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)
                    dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)
                    dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)
                    
                    dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)
                    dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)
                    dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)

                    elapsed_time = datetime.datetime.now() - start_time
                    
                    d_loss = 0.5 * np.add(dA_loss, dB_loss)

                    self.d_A.trainable = False
                    self.d_B.trainable = False

                    #생성자는 앞서 컴파일된 결합 모델을 통해 동시에 훈련됨. 6개의 출력은 컴파일 단계에서 정의한 6개의 손실함수에 대응
                    g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, valid, imgs_A, imgs_B, imgs_A, imgs_B])

                    count += 1

                    print("[Epoch %d/%d] [Batch %d/%d] [Count %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s " \
                        % ( self.epoch, epoch+1,
                            21, batch_i,
                            channel,  count,
                            d_loss[0], 100*d_loss[1],
                            g_loss[0],
                            np.sum(g_loss[1:3]),
                            np.sum(g_loss[3:5]),
                            np.sum(g_loss[5:7]),
                            elapsed_time))
                

                # If at save interval => save generated image samples
                if batch_i % sample_interval == 0:
                    self.combined.save_weights(os.path.join(self.RUN_FOLDER, 'weights/weights-%d.h5' % (self.epoch)))
                    self.combined.save_weights(os.path.join(self.RUN_FOLDER, 'weights/weights.h5'))
                    self.save_model(self.RUN_FOLDER)


                if epoch % sample_interval == 0:
                    self.sample_images(batch_i, epoch)
                    self.save_raw(RUN_FOLDER, epoch, np.array(self.temp_R))
                    del self.temp_R[:]


    def sample_images(self, batch_i, epoch):
        
        r, c = 2, 3

        temp_A, temp_B = [], []

        for i in range(2):
            test_A = self.image_processing(self.test_x[0][i+4], 256)
            fake_B = self.g_AB.predict(test_A)
            reconstr_A = self.g_BA.predict(fake_B)

            test_A = np.squeeze(test_A, axis=0)
            fake_B = np.squeeze(fake_B, axis=0)
            reconstr_A = np.squeeze(reconstr_A, axis=0)

            temp_A.append([test_A, fake_B, reconstr_A])

        for i in range(2):
            test_B = self.image_processing(self.test_y[0][i], 256)
            fake_A = self.g_BA.predict(test_B)
            reconstr_B = self.g_AB.predict(fake_A)

            test_B = np.squeeze(test_B, axis=0)
            fake_A = np.squeeze(fake_A, axis=0)
            reconstr_B = np.squeeze(reconstr_B, axis=0)

            temp_B.append([test_B, fake_A, reconstr_B])

        for i in range(20):
            test_R = self.image_processing(self.test_x[0][i+4], 256)
            fake_R = self.g_AB.predict(test_R)
            reconstr_R = np.squeeze(self.g_BA.predict(fake_R), axis=0)
            # fake_R = np.squeeze(self.g_AB.predict(test_R), axis=0) # (256, 256, 1)
                   
            self.temp_R.append(reconstr_R)  

        print(test_A.shape)

        gen_imgs = np.concatenate([test_A, fake_B, reconstr_A, test_B, fake_A, reconstr_B])

        gen_imgs = np.clip(gen_imgs, 0, 1)

        titles = ['Original', 'Translated', 'Reconstructed', 'ID']
        fig, axs = plt.subplots(r, c, figsize=(25,12.5))
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow(gen_imgs[cnt], cmap='gray')
                axs[i, j].set_title(titles[j])
                axs[i,j].axis('off')
                cnt += 1
        fig.savefig(os.path.join(self.RUN_FOLDER ,"images/%d_%d_%d.png" % (i, epoch, batch_i)))
        plt.close()


    def plot_model(self):
        plot_model(self.combined, to_file=os.path.join(self.RUN_FOLDER ,'viz/combined.png'), show_shapes = True, show_layer_names = True)
        plot_model(self.d_A, to_file=os.path.join(self.RUN_FOLDER ,'viz/d_A.png'), show_shapes = True, show_layer_names = True)
        plot_model(self.d_B, to_file=os.path.join(self.RUN_FOLDER ,'viz/d_B.png'), show_shapes = True, show_layer_names = True)
        plot_model(self.g_BA, to_file=os.path.join(self.RUN_FOLDER ,'viz/g_BA.png'), show_shapes = True, show_layer_names = True)
        plot_model(self.g_AB, to_file=os.path.join(self.RUN_FOLDER ,'viz/g_AB.png'), show_shapes = True, show_layer_names = True)


    def save(self, folder):

        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:
            pkl.dump([
                self.input_dim
                ,  self.learning_rate
                ,  self.buffer_max_length
                ,  self.lambda_validation
                ,  self.lambda_reconstr
                ,  self.lambda_id
                ,  self.gen_n_filters
                ,  self.disc_n_filters
                ], f)

        # self.plot_model(folder)
    
    def save_raw(self, run_folder,  epoch, images):
        path = run_folder + 'raws/' + ('Epochs%d.raw'%epoch)
        self.vx.NumpyArraytoVoxel_float(images)
        self.vx.DeNormalizeMM(self.original_image[0]) # [0]의 이유는 fake를 만들 때 train_x[0]을 했기 때문
        
        self.vx.WriteToRaw(path) # 정규화 풀어준 후 저장


    def save_model(self, run_folder):
        self.combined.save(os.path.join(run_folder, 'weights/model.h5')  )  
        self.d_A.save(os.path.join(run_folder, 'weights/d_A.h5') )
        self.d_B.save(os.path.join(run_folder, 'weights/d_B.h5') )
        self.g_BA.save(os.path.join(run_folder, 'weights/g_BA.h5')  )
        self.g_AB.save(os.path.join(run_folder, 'weights/g_AB.h5') )

    def load_weights(self, filepath):
        self.combined.load_weights(filepath)
        self.g_AB.load_weights(os.path.join(RUN_FOLDER + 'weights/g_AB_weights.h5'))
        self.g_BA.load_weights(os.path.join(RUN_FOLDER + 'weights/g_BA_weights.h5'))
        self.d_A.load_weights(os.path.join(RUN_FOLDER + 'weights/d_A_weights.h5'))
        self.d_B.load_weights(os.path.join(RUN_FOLDER + 'weights/d_B_weights.h5'))
    
    def image_processing(self, img, n_dim):
        imgs = Image.fromarray(img.astype(np.float32))
        imgs = np.array(imgs.resize((n_dim, n_dim)))

        imgs = np.expand_dims(imgs, axis=0)
        imgs = np.expand_dims(imgs, axis=3)
        return imgs


    def train_start(self): # 128, epoch, batch_size
        data_x, data_y = self.read_raw()

        self.train_x, self.test_x = data_x[:22], data_x[22:]
        self.train_y, self.test_y = data_y[:22], data_y[22:]

        self.train()

Path = './data'

cyclegan = CycleGAN(Path
    , RUN_FOLDER
    , Epochs = 900
    , input_dim = (256,256,1)
    , learning_rate = 0.0002
    , buffer_max_length = 50
    , lambda_validation = 1
    , lambda_reconstr = 10
    , lambda_id = 2
    , gen_n_filters = 32
    , disc_n_filters = 32
    )



cyclegan.train_start()
